{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, numpy as np, pandas as pd\n",
    "import seaborn as sns, matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tweepy.api import API\n",
    "from tweepy.auth import OAuthHandler\n",
    "from tweepy.cursor import Cursor\n",
    "from tweepy.streaming import Stream, StreamListener\n",
    "from pprint import PrettyPrinter, pformat, pprint\n",
    "from TweepyCredentials import ACCESS_TOKEN, ACCESS_TOKEN_SECRET, CONSUMER_KEY, CONSUMER_KEY_SECRET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "sns.set_style('whitegrid')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Authenticator():\n",
    "    '''A class that authenticates out API call.'''\n",
    "    \n",
    "    def authenticate_tweets(self):\n",
    "        auth = OAuthHandler(CONSUMER_KEY, CONSUMER_KEY_SECRET)\n",
    "        auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "        return auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyListener(StreamListener):\n",
    "    '''A custom listener class inheriting from tweepy StreamListener class methods that print/write streamed tweets.'''\n",
    "    \n",
    "    def __init__(self, tweet_file):\n",
    "        self.tweet_file = tweet_file\n",
    "        self.auth = Authenticator().authenticate_tweets()\n",
    "    \n",
    "    def on_data(self, status):\n",
    "        try:\n",
    "            # main goal is to append tweets to a list or write the tweets to a file\n",
    "            tweet = json.loads(status)\n",
    "            with open(self.tweet_file, 'a') as tf:\n",
    "                tf.write(tweet['text'])\n",
    "        except BaseException as e:\n",
    "            print('Error: ', str(e))\n",
    "\n",
    "    def on_error(self, error):\n",
    "        print(error)\n",
    "        if error == 420:\n",
    "            return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterClient():\n",
    "    '''A class to get various data from a specified user'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.auth = Authenticator().authenticate_tweets()\n",
    "        self.client = API(self.auth)\n",
    "        \n",
    "    def get_user_tweets(self, num_tweets, id_):\n",
    "        tweets = []\n",
    "        for tweet in Cursor(self.client.user_timeline, id_).items(num_tweets):\n",
    "            tweets.append(tweet._json)\n",
    "            \n",
    "        delete = []\n",
    "        for tweet in tweets:\n",
    "            for key, value in tweet.items():\n",
    "                if value in [None, False]:\n",
    "                    delete.append(key)    \n",
    "\n",
    "        for key in delete:\n",
    "            if key in tweet:\n",
    "                del tweet[key]\n",
    "            else:\n",
    "                pass    \n",
    "            \n",
    "        return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment of 100 tweets: \n",
      "\n",
      "                                                Tweet Sentiment\n",
      "0   RT @parscale: Joe Biden is a train wreck on il...  Negative\n",
      "1   RT @TimMurtaugh: Joe Biden would set an extrem...  Positive\n",
      "2   RT @kayleighmcenany: Sleepy Joe is SO CONFUSED...  Negative\n",
      "3                             https://t.co/HGMNIEImFT  Positive\n",
      "4   I must say, that was a VERY boring debate. Bid...  Negative\n",
      "..                                                ...       ...\n",
      "95  Many Republican Senators want me to Veto the F...  Positive\n",
      "96  RT @realDonaldTrump: â€œNancy Pelosi all of a su...  Negative\n",
      "97  RT @TwitterMoments: The NBA has suspended the ...  Negative\n",
      "98  RT @CDCemergency: Develop a plan for home-base...  Positive\n",
      "99  RT @CDCgov: Everyone can help prevent the spre...  Positive\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "        \n",
    "    from nltk.corpus import stopwords\n",
    "    import nltk\n",
    "    \n",
    "    stopwords = set(stopwords.words('english'))\n",
    "    \n",
    "    client = TwitterClient()\n",
    "    \n",
    "    tweets = client.get_user_tweets(100, 'realDonaldTrump')\n",
    "    tweet_text = [tweet['text'] for tweet in tweets]\n",
    "#     retweet_count = [tweet['retweet_count'] for tweet in tweets]\n",
    "#     created_at = [tweet['created_at'] for tweet in tweets]\n",
    "        \n",
    "    df = pd.DataFrame(data=tweet_text, columns=['Tweet'])\n",
    "#     df['Created At'] = created_at\n",
    "#     df['Retweets'] = retweet_count\n",
    "\n",
    "#     fig = plt.figure(figsize=(15,4))\n",
    "#     axes = fig.add_axes([0,0,1,1])\n",
    "#     axes.plot(df['Created At'][:100], df['Retweets'][:100], label = 'Retweet trend with time')\n",
    "\n",
    "\n",
    "# let's tokenize every tweet that we have got through out TwitterClient\n",
    "\n",
    "    tokenized_tweets = []\n",
    "    \n",
    "    for tweet in tweet_text:\n",
    "        tokenized_tweets.append(nltk.word_tokenize(tweet))\n",
    "        \n",
    "#     print(tokenized_tweets[0])\n",
    "\n",
    "# let's remove noise from each tweet token\n",
    "\n",
    "    cleaned_tokens = []\n",
    "    \n",
    "    for tweet_tokens in tokenized_tweets:\n",
    "        cleaned_tokens.append(remove_noise(tweet_tokens, stopwords))\n",
    "        \n",
    "#     print(len(cleaned_tokens[0]))\n",
    "\n",
    "# let's run our trained classifier\n",
    "\n",
    "    sentiments = []\n",
    "    \n",
    "    for tweet_tokens in cleaned_tokens:\n",
    "        # below code classifies a single tweet based on its tokens\n",
    "        sentiments.append(classifier.classify(dict([token, True] for token in tweet_tokens)))\n",
    "        \n",
    "        \n",
    "    print('The sentiment of {} tweets: \\n'.format(len(sentiments)))\n",
    "    \n",
    "    df['Sentiment'] = [sentiment for sentiment in sentiments]\n",
    "    \n",
    "    print(df[['Tweet', 'Sentiment']])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
