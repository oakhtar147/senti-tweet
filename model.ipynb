{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Downloading Twitter Data\n",
    "\n",
    "Running the command below from the Python interpreter downloads and stores the tweets locally. Once the samples are downloaded, they are available for your use.\n",
    "\n",
    "You will use the negative and positive tweets to train your model on sentiment analysis later in the tutorial. The tweets with no sentiments will be used to test your model.\n",
    "\n",
    "If you would like to use your own dataset, you can gather tweets from a specific time period, user, or hashtag by using the Twitter API.\n",
    "\n",
    "Now that you’ve imported NLTK and downloaded the sample tweets, exit the interactive session by entering in exit(). You are ready to import the tweets and begin processing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"twitter_samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Tokenizing the Data\n",
    "\n",
    "Language in its original form cannot be accurately processed by a machine, so you need to process the language to make it easier for the machine to understand. The first part of making sense of the data is through a process called tokenization, or splitting strings into smaller parts called tokens.\n",
    "\n",
    "A token is a sequence of characters in text that serves as a unit. Based on how you create the tokens, they may consist of words, emoticons, hashtags, links, or even individual characters. A basic way of breaking language into tokens is by splitting the text based on whitespace and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "neutral_tweets = twitter_samples.strings('tweets.20150430-223406.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strings() method of twitter_samples will print all of the tweets within a dataset as strings. Setting the different tweet collections as a variable will make processing and testing easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are ready to use NLTK’s tokenizers. NLTK provides a default tokenizer for tweets with the .tokenized() method. Add a line to create an object that tokenizes the positive_tweets.json dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "print(tweet_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the .tokenized() method returns special characters such as @ and _. These characters will be removed through regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Normalizing the Data\n",
    "\n",
    "Words have different forms—for instance, “ran”, “runs”, and “running” are various forms of the same verb, “run”. Depending on the requirement of your analysis, all of these versions may need to be converted to the same form, “run”. Normalization in NLP is the process of converting a word to its canonical form.\n",
    "\n",
    "Normalization helps group together words with the same meaning but different forms. Without normalization, “ran”, “runs”, and “running” would be treated as different words, even though you may want them to be treated as the same word. In this section, you explore stemming and lemmatization, which are two popular techniques of normalization.\n",
    "\n",
    "Stemming is a process of removing affixes from a word. Stemming, working with only simple verb forms, is a heuristic process that removes the ends of words.\n",
    "\n",
    "In this tutorial you will use the process of lemmatization, which normalizes a word with the context of vocabulary and morphological analysis of words in text. The lemmatization algorithm analyzes the structure of the word and its context to convert it to a normalized form. Therefore, it comes at a cost of speed. A comparison of stemming and lemmatization ultimately comes down to a trade off between speed and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Before running a lemmatizer, you need to determine the context for each word in your text. This is achieved by a tagging algorithm, which assesses the relative position of a word in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#FollowFriday', 'JJ'),\n",
      " ('@France_Inte', 'NNP'),\n",
      " ('@PKuchly57', 'NNP'),\n",
      " ('@Milipol_Paris', 'NNP'),\n",
      " ('for', 'IN'),\n",
      " ('being', 'VBG'),\n",
      " ('top', 'JJ'),\n",
      " ('engaged', 'VBN'),\n",
      " ('members', 'NNS'),\n",
      " ('in', 'IN'),\n",
      " ('my', 'PRP$'),\n",
      " ('community', 'NN'),\n",
      " ('this', 'DT'),\n",
      " ('week', 'NN'),\n",
      " (':)', 'NN')]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from pprint import pprint \n",
    "\n",
    "print(pprint(pos_tag(tweet_tokens[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "    \n",
    "From the list of tags, here is the list of the most common items and their meaning:\n",
    "<ul>\n",
    "NNP: Noun, proper, singular\n",
    "</ul>\n",
    "<ul>\n",
    "NN: Noun, common, singular or mass\n",
    "</ul>    \n",
    "<ul>\n",
    "IN: Preposition or conjunction, subordinating\n",
    "</ul>\n",
    "<ul>\n",
    "VBG: Verb, gerund or present participle\n",
    "</ul>\n",
    "<ul>\n",
    "VBN: Verb, past participle    \n",
    "</ul>    \n",
    "\n",
    "In general, if a tag starts with NN, the word is a noun and if it starts with VB, the word is a verb.\n",
    "\n",
    "To incorporate this into a function that normalizes a sentence, you should first generate the tags for each token in the text, and then lemmatize each word using the tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi',\n",
      " 'my',\n",
      " 'name',\n",
      " 'be',\n",
      " '@',\n",
      " 'Osama',\n",
      " 'and',\n",
      " 'I',\n",
      " 'be',\n",
      " 'type',\n",
      " 'very',\n",
      " 'fast',\n",
      " 'and',\n",
      " 'be',\n",
      " 'fast',\n",
      " 'mean',\n",
      " 'make',\n",
      " 'more',\n",
      " 'mistake']\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    \n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sent = []\n",
    "    \n",
    "    for word, tag in tagged_tokens:\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sent.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sent\n",
    "        \n",
    "\n",
    "print(pprint(lemmatize(nltk.word_tokenize('Hi my name is @Osama and I am typing very fast and being fast means making more mistakes')))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that the verb 'being' changes to its root form, 'be' and 'running' change to 'run'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 — Removing Noise from the Data\n",
    "\n",
    "In this step, you will remove noise from the dataset. Noise is any part of the text that does not add meaning or information to data.\n",
    "\n",
    "Noise is specific to each project, so what constitutes noise in one project may not be in a different project. For instance, the most common words in a language are called stop words. Some examples of stop words are “is”, “the”, and “a”. They are generally irrelevant when processing language, unless a specific use case warrants their inclusion.\n",
    "\n",
    "In this tutorial, you will use regular expressions in Python to search for and remove these items:\n",
    "\n",
    "**Hyperlinks** - All hyperlinks in Twitter are converted to the URL shortener t.co. Therefore, keeping them in the text processing would not add any value to the analysis.\n",
    "\n",
    "**Twitter handles in replies** - These Twitter usernames are preceded by a @ symbol, which does not convey any meaning.\n",
    "\n",
    "**Punctuation and special characters** - While these often provide context to textual data, this context is often difficult to process. For simplicity, you will remove all punctuation and special characters from tweets.\n",
    "\n",
    "To remove hyperlinks, you need to first search for a substring that matches a URL starting with http:// or https://, followed by letters, numbers, or special characters. Once a pattern is matched, the .sub() method replaces it with an empty string.\n",
    "\n",
    "Since we will normalize word forms within the remove_noise() function, you can comment out the lemmatize_sentence() function from the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday', 'top', 'engaged', 'member', 'community', 'week', ':)']\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import re, string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def remove_noise(tokens, stop_words):\n",
    "    cleaned_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "            token = re.sub(pattern = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', repl = '', string = token)\n",
    "            token = re.sub(pattern = \"(@[A-Za-z0-9_]+)\", repl = '', string = token)\n",
    "            \n",
    "            if len(token) > 0 and (token not in string.punctuation) and (token.lower() not in stop_words):\n",
    "                cleaned_tokens.append(token)\n",
    "    \n",
    "    cleaned_lemmatized_tokens = lemmatize(cleaned_tokens)\n",
    "    \n",
    "    return cleaned_lemmatized_tokens\n",
    "\n",
    "print(pprint(remove_noise(tweet_tokens[0], stopwords)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding to the modeling exercise in the next step, use the remove_noise() function to clean the positive and negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)']\n",
      "['#FollowFriday', 'top', 'engaged', 'member', 'community', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "cleaned_positive_tweets = [remove_noise(token, stopwords) for token in positive_tweet_tokens]\n",
    "cleaned_negative_tweets = [remove_noise(token, stopwords) for token in negative_tweet_tokens]\n",
    "\n",
    "print(positive_tweet_tokens[0]) \n",
    "print(cleaned_positive_tweets[0]) # print out a random cleaned negative tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 — Determining Word Density\n",
    "\n",
    "The most basic form of analysis on textual data is to take out the word frequency. A single tweet is too small of an entity to find out the distribution of words, hence, the analysis of the frequency of words would be done on all positive tweets.\n",
    "\n",
    "The following snippet defines a generator function, named get_all_words, that takes a list of tweets as an argument to provide a list of words in all of the tweet tokens joined.\n",
    "\n",
    "Yield is a lazy iterator and it does not drain the memory unlike 'return', that is why it is used for iterating over such large\n",
    "datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object word_density at 0x000001F1833DE348>\n"
     ]
    }
   ],
   "source": [
    "def word_density(cleaned_tweet_list):\n",
    "    \n",
    "    for tokens in cleaned_tweet_list:\n",
    "        for token in tokens:\n",
    "                yield token\n",
    "  \n",
    "all_pos_words = word_density(cleaned_positive_tweets)\n",
    "print(all_pos_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have compiled all words in the sample of tweets, you can find out which are the most common words using the FreqDist class of NLTK.\n",
    "\n",
    "The .most_common() method lists the words which occur most frequently in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(':)', 3691),\n",
      " (':-)', 701),\n",
      " (':D', 658),\n",
      " ('follow', 338),\n",
      " ('...', 290),\n",
      " ('love', 242),\n",
      " ('day', 235),\n",
      " ('get', 234),\n",
      " ('u', 228),\n",
      " ('like', 220)]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "print(pprint(freq_dist_pos.most_common(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this data, you can see that emoticon entities form some of the most common parts of positive tweets. Before proceeding to the next step, make sure you comment out the last line of the script that prints the top ten tokens.\n",
    "\n",
    "To summarize, you extracted the tweets from nltk, tokenized, normalized, and cleaned up the tweets for using in the model. Finally, you also looked at the frequencies of tokens in the data and checked the frequencies of the top ten tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 — Preparing Data for the Model\n",
    "\n",
    "\n",
    "Sentiment analysis is a process of identifying an attitude of the author on a topic that is being written about. You will create a training data set to train a model. It is a supervised learning machine learning process, which requires you to associate each dataset with a “sentiment” for training. In this tutorial, your model will use the “positive” and “negative” sentiments.\n",
    "\n",
    "Sentiment analysis can be used to categorize text into a variety of sentiments. For simplicity and availability of the training dataset, this tutorial helps you train your model in only two categories, positive and negative.\n",
    "\n",
    "A model is a description of a system using rules and equations. It may be as simple as an equation which predicts the weight of a person, given their height. A sentiment analysis model that you will build would associate tweets with a positive or a negative sentiment. You will need to split your dataset into two parts. The purpose of the first part is to build the model, whereas the next part tests the performance of the model.\n",
    "\n",
    "In the data preparation step, you will prepare the data for sentiment analysis by converting tokens to the dictionary form and then split the data for training and testing purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Tokens to a Dictionary\n",
    "\n",
    "First, you will prepare the data to be fed into the model. You will use the Naive Bayes classifier in NLTK to perform the modeling exercise. Notice that the model requires not just a list of words in a tweet, but a Python dictionary with words as keys and True as values. The following function makes a generator function to change the format of the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_dict(cleaned_tweet_tokens):\n",
    "    \n",
    "    for tweet_tokens in cleaned_tweet_tokens:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "        \n",
    "positive_tokens_for_model = token_dict(positive_tweet_tokens)\n",
    "negative_tokens_for_model = token_dict(negative_tweet_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attach a Positive or Negative label to each tweet. It then creates a dataset by joining the positive and negative tweets.\n",
    "\n",
    "By default, the data contains all positive tweets followed by all negative tweets in sequence. When training the model, you should provide a sample of your data that does not contain any bias. To avoid bias, you’ve added code to randomly arrange the data using the .shuffle() method of random.\n",
    "\n",
    "Finally, the code splits the shuffled data into a ratio of 70:30 for training and testing, respectively. Since the number of tweets is 10000, you can use the first 7000 tweets from the shuffled dataset for training the model and the final 3000 for testing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as r\n",
    "\n",
    "positive_dataset = [(token, 'Positive') for token in positive_tokens_for_model]\n",
    "negative_dataset = [(token, 'Negative') for token in negative_tokens_for_model]\n",
    "\n",
    "dataset = positive_dataset + negative_dataset\n",
    "\n",
    "r.shuffle(dataset)\n",
    "\n",
    "# splitting the train/test data\n",
    "\n",
    "train_data = dataset[:7000]\n",
    "test_data = dataset[7000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7 — Building and Testing the Model\n",
    "\n",
    "Finally, you can use the NaiveBayesClassifier class to build the model. Use the .train() method to train the model and the .accuracy() method to test the model on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence is: 0.9946666666666667\n",
      "\n",
      "Most Informative Features\n",
      "                      :) = True           Positi : Negati =    974.7 : 1.0\n",
      "                     sad = True           Negati : Positi =     30.9 : 1.0\n",
      "                 arrived = True           Positi : Negati =     27.0 : 1.0\n",
      "                  THANKS = True           Negati : Positi =     24.2 : 1.0\n",
      "                  Thanks = True           Positi : Negati =     23.2 : 1.0\n",
      "                    miss = True           Negati : Positi =     19.5 : 1.0\n",
      "                    THAT = True           Negati : Positi =     18.1 : 1.0\n",
      "                  FOLLOW = True           Negati : Positi =     17.4 : 1.0\n",
      "                    MUCH = True           Negati : Positi =     17.4 : 1.0\n",
      "                     TOO = True           Negati : Positi =     14.9 : 1.0\n",
      "                    poor = True           Negati : Positi =     14.7 : 1.0\n",
      "                   Thank = True           Positi : Negati =     14.1 : 1.0\n",
      "               followers = True           Positi : Negati =     14.0 : 1.0\n",
      "                     See = True           Positi : Negati =     13.9 : 1.0\n",
      "                    damn = True           Negati : Positi =     13.3 : 1.0\n",
      "                  thanks = True           Positi : Negati =     12.7 : 1.0\n",
      "                    glad = True           Positi : Negati =     12.0 : 1.0\n",
      "                     Job = True           Positi : Negati =     12.0 : 1.0\n",
      "                 welcome = True           Positi : Negati =     11.9 : 1.0\n",
      "                   tired = True           Negati : Positi =     11.7 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(f'Confidence is: {classify.accuracy(classifier, test_data)}\\n')\n",
    "print(classifier.show_most_informative_features(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
